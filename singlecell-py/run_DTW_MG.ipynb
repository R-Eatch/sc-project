{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8964aec2-71d5-4966-83b4-16e50adfbe0c",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] Unable to synchronously open file (unable to open file: name = '../../M-MG/1.subset/M-MG_cleaned.h5ad', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m path_r \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../../R-MG/1.subset/R-MG_cleaned.h5ad\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     19\u001b[0m path_s \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../../S-MG/1.subset/S-MG_cleaned.h5ad\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 21\u001b[0m adata_m \u001b[38;5;241m=\u001b[39m \u001b[43msc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_h5ad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m adata_r \u001b[38;5;241m=\u001b[39m sc\u001b[38;5;241m.\u001b[39mread_h5ad(path_r)\n\u001b[0;32m     23\u001b[0m adata_s \u001b[38;5;241m=\u001b[39m sc\u001b[38;5;241m.\u001b[39mread_h5ad(path_s)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\py310\\lib\\site-packages\\anndata\\_io\\h5ad.py:234\u001b[0m, in \u001b[0;36mread_h5ad\u001b[1;34m(filename, backed, as_sparse, as_sparse_fmt, chunk_size)\u001b[0m\n\u001b[0;32m    226\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    227\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrently only `X` and `raw/X` can be read as sparse.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    228\u001b[0m         )\n\u001b[0;32m    230\u001b[0m rdasp \u001b[38;5;241m=\u001b[39m partial(\n\u001b[0;32m    231\u001b[0m     read_dense_as_sparse, sparse_format\u001b[38;5;241m=\u001b[39mas_sparse_fmt, axis_chunk\u001b[38;5;241m=\u001b[39mchunk_size\n\u001b[0;32m    232\u001b[0m )\n\u001b[1;32m--> 234\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mh5py\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    236\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcallback\u001b[39m(func, elem_name: \u001b[38;5;28mstr\u001b[39m, elem, iospec):\n\u001b[0;32m    237\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m iospec\u001b[38;5;241m.\u001b[39mencoding_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manndata\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m elem_name\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\py310\\lib\\site-packages\\h5py\\_hl\\files.py:562\u001b[0m, in \u001b[0;36mFile.__init__\u001b[1;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[0;32m    553\u001b[0m     fapl \u001b[38;5;241m=\u001b[39m make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,\n\u001b[0;32m    554\u001b[0m                      locking, page_buf_size, min_meta_keep, min_raw_keep,\n\u001b[0;32m    555\u001b[0m                      alignment_threshold\u001b[38;5;241m=\u001b[39malignment_threshold,\n\u001b[0;32m    556\u001b[0m                      alignment_interval\u001b[38;5;241m=\u001b[39malignment_interval,\n\u001b[0;32m    557\u001b[0m                      meta_block_size\u001b[38;5;241m=\u001b[39mmeta_block_size,\n\u001b[0;32m    558\u001b[0m                      \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    559\u001b[0m     fcpl \u001b[38;5;241m=\u001b[39m make_fcpl(track_order\u001b[38;5;241m=\u001b[39mtrack_order, fs_strategy\u001b[38;5;241m=\u001b[39mfs_strategy,\n\u001b[0;32m    560\u001b[0m                      fs_persist\u001b[38;5;241m=\u001b[39mfs_persist, fs_threshold\u001b[38;5;241m=\u001b[39mfs_threshold,\n\u001b[0;32m    561\u001b[0m                      fs_page_size\u001b[38;5;241m=\u001b[39mfs_page_size)\n\u001b[1;32m--> 562\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mmake_fid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muserblock_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfcpl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mswmr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mswmr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    564\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(libver, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    565\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_libver \u001b[38;5;241m=\u001b[39m libver\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\py310\\lib\\site-packages\\h5py\\_hl\\files.py:235\u001b[0m, in \u001b[0;36mmake_fid\u001b[1;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m swmr \u001b[38;5;129;01mand\u001b[39;00m swmr_support:\n\u001b[0;32m    234\u001b[0m         flags \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mACC_SWMR_READ\n\u001b[1;32m--> 235\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mh5f\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfapl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    237\u001b[0m     fid \u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mopen(name, h5f\u001b[38;5;241m.\u001b[39mACC_RDWR, fapl\u001b[38;5;241m=\u001b[39mfapl)\n",
      "File \u001b[1;32mh5py\\_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\h5f.pyx:102\u001b[0m, in \u001b[0;36mh5py.h5f.open\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] Unable to synchronously open file (unable to open file: name = '../../M-MG/1.subset/M-MG_cleaned.h5ad', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import scanpy as sc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from dtw import dtw  # 使用dtw库进行动态时间规整\n",
    "\n",
    "##############################################################################\n",
    "# 新增四个开关变量，用于控制是否进行MG / AG 的 sample / stage 分析\n",
    "##############################################################################\n",
    "do_MG_sample = True   # 是否进行MG数据的sample分析\n",
    "do_MG_stage  = True   # 是否进行MG数据的stage分析\n",
    "do_AG_sample = True   # 是否进行AG数据的sample分析\n",
    "do_AG_stage  = True   # 是否进行AG数据的stage分析\n",
    "\n",
    "# 预设全局变量，可以调整是否使用三物种共有HVGs\n",
    "use_three_species_hvgs = True  # 设置为 True 使用三物种共有HVGs，False 使用样本间共有HVGs\n",
    "\n",
    "##############################################################################\n",
    "# 1. 读取三物种数据，并添加必要的 obs 信息 (MG部分)\n",
    "##############################################################################\n",
    "path_m = \"../../M-MG/1.subset/M-MG_cleaned.h5ad\"\n",
    "path_r = \"../../R-MG/1.subset/R-MG_cleaned.h5ad\"\n",
    "path_s = \"../../S-MG/1.subset/S-MG_cleaned.h5ad\"\n",
    "\n",
    "adata_m = sc.read_h5ad(path_m)\n",
    "adata_r = sc.read_h5ad(path_r)\n",
    "adata_s = sc.read_h5ad(path_s)\n",
    "\n",
    "# 为每个数据集添加物种标签\n",
    "adata_m.obs['species'] = \"M\"\n",
    "adata_r.obs['species'] = \"R\"\n",
    "adata_s.obs['species'] = \"S\"\n",
    "\n",
    "# 将三个数据集放入字典，方便后续调用\n",
    "adata_dict = {\"M\": adata_m, \"R\": adata_r, \"S\": adata_s}\n",
    "\n",
    "##############################################################################\n",
    "# 预设三个物种样本的时间顺序（手动定义）\n",
    "##############################################################################\n",
    "R_order = ['R-MG-E17', 'R-MG-E23','R-MG-P1' ,'R-MG-8WK-1',\n",
    "  'R-MG-23WK-3', 'R-MG-23WK-4','R-MG-GES12', 'R-MG-GES17', 'R-MG-GES23','R-MG-LA','R-MG-LA-2']\n",
    "S_order = ['S-MG-P7',\n",
    "           'S-MG-P20', \n",
    "           'S-MG-3MTH',\n",
    "           'S-MG-8M-3',\n",
    "           'S-MG-8M-4',\n",
    "           'S-MG-GES14',\n",
    "           'S-MG-LA-1',\n",
    "            'S-MG-LA-2']\n",
    "M_order = ['M-MG-E13_5',\n",
    "           'M-MG-E16_5', 'M-MG-P1','M-MG-3WK-1',\n",
    "           'M-MG-3WK-2',\n",
    "           'M-MG-8WK-1',\n",
    "           'M-MG-8WK-2',\n",
    "           'M-MG-GES13_5',\n",
    "           'M-MG-GES16_5',\n",
    "           'M-MG-LA-1',\n",
    "           'M-MG-LA-2']\n",
    "\n",
    "##############################################################################\n",
    "# 2 & 3. 针对各物种对，进行“样本”级别的Manhattan距离 & DTW分析\n",
    "#        以及绘制曼哈顿距离热图 (样本维度)\n",
    "##############################################################################\n",
    "if do_MG_sample:\n",
    "    species_pairs = [(\"M\", \"R\"), (\"M\", \"S\"), (\"R\", \"S\")]\n",
    "\n",
    "    sample_distance_summary = {}\n",
    "    dtw_summary = {}\n",
    "\n",
    "    for sp1, sp2 in species_pairs:\n",
    "        print(f\"正在处理物种对: {sp1} vs {sp2}\", flush=True)\n",
    "        \n",
    "        # 根据全局变量判断是否选择三物种共享的 HVGs\n",
    "        if use_three_species_hvgs:\n",
    "            hvgs_sp1 = adata_dict[sp1].var_names[adata_dict[sp1].var['highly_variable']]\n",
    "            hvgs_sp2 = adata_dict[sp2].var_names[adata_dict[sp2].var['highly_variable']]\n",
    "            hvgs_sp3 = adata_dict[\"S\"].var_names[adata_dict[\"S\"].var['highly_variable']]\n",
    "            common_hvgs = list(set(hvgs_sp1).intersection(hvgs_sp2).intersection(hvgs_sp3))\n",
    "            print(f\"{sp1} vs {sp2} vs S 共有 HVGs 数量: {len(common_hvgs)}\", flush=True)\n",
    "        else:\n",
    "            hvgs_sp1 = adata_dict[sp1].var_names[adata_dict[sp1].var['highly_variable']]\n",
    "            hvgs_sp2 = adata_dict[sp2].var_names[adata_dict[sp2].var['highly_variable']]\n",
    "            common_hvgs = list(set(hvgs_sp1).intersection(set(hvgs_sp2)))\n",
    "            print(f\"{sp1} vs {sp2} 共有 HVGs 数量: {len(common_hvgs)}\", flush=True)\n",
    "        \n",
    "        # 针对当前 pair，从原始数据中只保留共同 HVGs\n",
    "        a1 = adata_dict[sp1][:, common_hvgs].copy()\n",
    "        a2 = adata_dict[sp2][:, common_hvgs].copy()\n",
    "        \n",
    "        # 合并两个数据集，利用 concatenate 保证批次区分\n",
    "        adata_pair = a1.concatenate(\n",
    "            a2,\n",
    "            batch_key=\"species_batch\",\n",
    "            batch_categories=[sp1, sp2],\n",
    "            join='inner'\n",
    "        )\n",
    "        \n",
    "        # 根据预设的样本顺序，确定各物种样本的顺序\n",
    "        if sp1 == \"R\":\n",
    "            sp1_samples = R_order\n",
    "        elif sp1 == \"S\":\n",
    "            sp1_samples = S_order\n",
    "        elif sp1 == \"M\":\n",
    "            sp1_samples = M_order\n",
    "        else:\n",
    "            sp1_samples = adata_pair.obs.loc[adata_pair.obs['species_batch'] == sp1, 'sample'].unique().tolist()\n",
    "            \n",
    "        if sp2 == \"R\":\n",
    "            sp2_samples = R_order\n",
    "        elif sp2 == \"S\":\n",
    "            sp2_samples = S_order\n",
    "        elif sp2 == \"M\":\n",
    "            sp2_samples = M_order\n",
    "        else:\n",
    "            sp2_samples = adata_pair.obs.loc[adata_pair.obs['species_batch'] == sp2, 'sample'].unique().tolist()\n",
    "        \n",
    "        print(f\"使用的 {sp1} 样本顺序: {sp1_samples}\", flush=True)\n",
    "        print(f\"使用的 {sp2} 样本顺序: {sp2_samples}\", flush=True)\n",
    "        \n",
    "        # 计算每个样本的均值表达向量\n",
    "        sp1_sample_means = {}\n",
    "        for sample in sp1_samples:\n",
    "            mask = (adata_pair.obs['species_batch'] == sp1) & (adata_pair.obs['sample'] == sample)\n",
    "            sp1_sample_means[sample] = np.asarray(adata_pair[mask].X.mean(axis=0)).flatten()\n",
    "        \n",
    "        sp2_sample_means = {}\n",
    "        for sample in sp2_samples:\n",
    "            mask = (adata_pair.obs['species_batch'] == sp2) & (adata_pair.obs['sample'] == sample)\n",
    "            sp2_sample_means[sample] = np.asarray(adata_pair[mask].X.mean(axis=0)).flatten()\n",
    "        \n",
    "        # 计算样本间的Manhattan距离（绝对差值之和）\n",
    "        sample_distance_mat = pd.DataFrame(index=sp1_samples, columns=sp2_samples, dtype=float)\n",
    "        for sample1 in sp1_samples:\n",
    "            for sample2 in sp2_samples:\n",
    "                print(f\"正在计算样本: {sample1} vs {sample2}\", flush=True)\n",
    "                vec1 = sp1_sample_means[sample1]\n",
    "                vec2 = sp2_sample_means[sample2]\n",
    "                manhattan_dist = np.sum(np.abs(vec1 - vec2))\n",
    "                sample_distance_mat.loc[sample1, sample2] = manhattan_dist\n",
    "        \n",
    "        # 保存Manhattan距离矩阵到 CSV\n",
    "        out_csv = f\"Manhattan_distance_{sp1}_vs_{sp2}_samples.csv\"\n",
    "        sample_distance_mat.to_csv(out_csv)\n",
    "        print(f\"保存 Manhattan 距离矩阵到 {out_csv}\", flush=True)\n",
    "        sample_distance_summary[f\"{sp1}_vs_{sp2}\"] = sample_distance_mat.copy()\n",
    "        \n",
    "        # -------------------------------\n",
    "        # DTW 分析：基于样本间 Manhattan 距离矩阵进行 DTW 对齐\n",
    "        # -------------------------------\n",
    "        seq_x = np.arange(len(sp1_samples))\n",
    "        seq_y = np.arange(len(sp2_samples))\n",
    "        \n",
    "        def cost_function(x, y):\n",
    "            i = int(x)\n",
    "            j = int(y)\n",
    "            return sample_distance_mat.iloc[i, j]\n",
    "        \n",
    "        dtw_distance, dtw_cost_matrix, dtw_acc_cost_matrix, dtw_path = dtw(seq_x, seq_y, dist=cost_function)\n",
    "        path_x, path_y = dtw_path\n",
    "        print(f\"DTW 距离 ({sp1} vs {sp2}):\", dtw_distance)\n",
    "        \n",
    "        dtw_summary[f\"{sp1}_vs_{sp2}\"] = {\n",
    "            \"dtw_distance\": dtw_distance,\n",
    "            \"path\": (path_x, path_y),\n",
    "            \"acc_cost_matrix\": dtw_acc_cost_matrix\n",
    "        }\n",
    "        \n",
    "        # 绘制DTW累积代价矩阵及最佳对齐路径 (在这里将\"累计代价\"改为\"Accumulated Cost\")\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        ax = sns.heatmap(dtw_acc_cost_matrix.T, cmap='viridis',\n",
    "                         cbar=True, cbar_kws={'label': 'Accumulated Cost'},\n",
    "                         xticklabels=sp1_samples, yticklabels=sp2_samples)\n",
    "        plt.xlabel(f\"{sp1} Samples (ordered)\")\n",
    "        plt.ylabel(f\"{sp2} Samples (ordered)\")\n",
    "        plt.title(f\"DTW Alignment ({sp1} vs {sp2}): Accumulated Cost Matrix\")\n",
    "        \n",
    "        # 在heatmap中绘制箭头，表示最优对齐路径 (optimal alignment path)\n",
    "        for k in range(len(path_x) - 1):\n",
    "            x0, y0 = path_x[k], path_y[k]\n",
    "            dx = path_x[k+1] - x0\n",
    "            dy = path_y[k+1] - y0\n",
    "            plt.arrow(x0 + 0.5, y0 + 0.5, dx, dy, color='black', \n",
    "                      head_width=0.3, head_length=0.3, length_includes_head=True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        out_fig = f\"DTW_alignment_heatmap_{sp1}_vs_{sp2}.png\"\n",
    "        plt.savefig(out_fig, dpi=300)\n",
    "        plt.show()\n",
    "        print(f\"保存 DTW 对齐热图到 {out_fig}\", flush=True)\n",
    "\n",
    "    # 绘制“样本”级曼哈顿距离热图 (在这里将\"曼哈顿距离\"改为\"Manhattan Distance\")\n",
    "    order_dict = {\"R\": R_order, \"S\": S_order, \"M\": M_order}\n",
    "    for sp1, sp2 in species_pairs:\n",
    "        sp1_samples = order_dict.get(sp1, adata_dict[sp1].obs['sample'].unique().tolist())\n",
    "        sp2_samples = order_dict.get(sp2, adata_dict[sp2].obs['sample'].unique().tolist())\n",
    "        sample_distance_mat = sample_distance_summary[f\"{sp1}_vs_{sp2}\"]\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        ax = sns.heatmap(sample_distance_mat, cmap='plasma',\n",
    "                         cbar_kws={'label': 'Manhattan Distance'},\n",
    "                         xticklabels=sp2_samples, yticklabels=sp1_samples)\n",
    "        plt.xlabel(f\"{sp2} Samples\")\n",
    "        plt.ylabel(f\"{sp1} Samples\")\n",
    "        plt.title(f\"Manhattan Distance Heatmap (samples): {sp1} vs {sp2}\")\n",
    "        plt.tight_layout()\n",
    "        out_fig = f\"Manhattan_distance_heatmap_{sp1}_vs_{sp2}_samples.png\"\n",
    "        plt.savefig(out_fig, dpi=300)\n",
    "        plt.show()\n",
    "        print(f\"保存曼哈顿距离热图 (样本)到 {out_fig}\", flush=True)\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "# 4. 基于 stage 分组进行分析（MG部分）\n",
    "##############################################################################\n",
    "if do_MG_stage:\n",
    "    species_pairs = [(\"M\", \"R\"), (\"M\", \"S\"), (\"R\", \"S\")]\n",
    "\n",
    "    for sp1, sp2 in species_pairs:\n",
    "        print(f\"正在处理物种对 (stage分组): {sp1} vs {sp2}\", flush=True)\n",
    "        \n",
    "        if use_three_species_hvgs:\n",
    "            hvgs_sp1 = adata_dict[sp1].var_names[adata_dict[sp1].var['highly_variable']]\n",
    "            hvgs_sp2 = adata_dict[sp2].var_names[adata_dict[sp2].var['highly_variable']]\n",
    "            hvgs_sp3 = adata_dict[\"S\"].var_names[adata_dict[\"S\"].var['highly_variable']]\n",
    "            common_hvgs = list(set(hvgs_sp1).intersection(hvgs_sp2).intersection(hvgs_sp3))\n",
    "            print(f\"{sp1} vs {sp2} vs S 共有 HVGs 数量: {len(common_hvgs)}\", flush=True)\n",
    "        else:\n",
    "            hvgs_sp1 = adata_dict[sp1].var_names[adata_dict[sp1].var['highly_variable']]\n",
    "            hvgs_sp2 = adata_dict[sp2].var_names[adata_dict[sp2].var['highly_variable']]\n",
    "            common_hvgs = list(set(hvgs_sp1).intersection(set(hvgs_sp2)))\n",
    "            print(f\"{sp1} vs {sp2} 共有 HVGs 数量: {len(common_hvgs)}\", flush=True)\n",
    "        \n",
    "        a1 = adata_dict[sp1][:, common_hvgs].copy()\n",
    "        a2 = adata_dict[sp2][:, common_hvgs].copy()\n",
    "        \n",
    "        adata_pair = a1.concatenate(\n",
    "            a2,\n",
    "            batch_key=\"species_batch\",\n",
    "            batch_categories=[sp1, sp2],\n",
    "            join='inner'\n",
    "        )\n",
    "\n",
    "        desired_stage_order = [\"stage0\", \"stage1\", \"stage2\", \"stage3\", \"stage4\"]\n",
    "        \n",
    "        sp1_all_stages = adata_pair.obs.loc[adata_pair.obs['species_batch'] == sp1, 'stage'].unique().tolist()\n",
    "        sp1_stages = [stage for stage in desired_stage_order if stage in sp1_all_stages]\n",
    "        \n",
    "        sp2_all_stages = adata_pair.obs.loc[adata_pair.obs['species_batch'] == sp2, 'stage'].unique().tolist()\n",
    "        sp2_stages = [stage for stage in desired_stage_order if stage in sp2_all_stages]\n",
    "\n",
    "        sp1_stage_means = {}\n",
    "        for stage in sp1_stages:\n",
    "            mask = (adata_pair.obs['species_batch'] == sp1) & (adata_pair.obs['stage'] == stage)\n",
    "            sp1_stage_means[stage] = np.asarray(adata_pair[mask].X.mean(axis=0)).flatten()\n",
    "        \n",
    "        sp2_stage_means = {}\n",
    "        for stage in sp2_stages:\n",
    "            mask = (adata_pair.obs['species_batch'] == sp2) & (adata_pair.obs['stage'] == stage)\n",
    "            sp2_stage_means[stage] = np.asarray(adata_pair[mask].X.mean(axis=0)).flatten()\n",
    "        \n",
    "        stage_distance_mat = pd.DataFrame(index=sp1_stages, columns=sp2_stages, dtype=float)\n",
    "        for stage1 in sp1_stages:\n",
    "            for stage2 in sp2_stages:\n",
    "                vec1 = sp1_stage_means[stage1]\n",
    "                vec2 = sp2_stage_means[stage2]\n",
    "                manhattan_dist = np.sum(np.abs(vec1 - vec2))\n",
    "                stage_distance_mat.loc[stage1, stage2] = manhattan_dist\n",
    "        \n",
    "        out_csv_stage = f\"Manhattan_distance_{sp1}_vs_{sp2}_stages.csv\"\n",
    "        stage_distance_mat.to_csv(out_csv_stage)\n",
    "        print(f\"保存曼哈顿距离矩阵 (stage) 到 {out_csv_stage}\", flush=True)\n",
    "        \n",
    "        # DTW 分析 (stage)\n",
    "        seq_x = np.arange(len(sp1_stages))\n",
    "        seq_y = np.arange(len(sp2_stages))\n",
    "        \n",
    "        def cost_function_stage(x, y):\n",
    "            i = int(x)\n",
    "            j = int(y)\n",
    "            return stage_distance_mat.iloc[i, j]\n",
    "        \n",
    "        dtw_distance, dtw_cost_matrix, dtw_acc_cost_matrix, dtw_path = dtw(seq_x, seq_y, dist=cost_function_stage)\n",
    "        path_x, path_y = dtw_path\n",
    "        print(f\"DTW 距离 (stage) ({sp1} vs {sp2}):\", dtw_distance)\n",
    "        \n",
    "        # 绘制 Accumulated Cost 矩阵 (stage)\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        ax = sns.heatmap(dtw_acc_cost_matrix.T, cmap='viridis',\n",
    "                         cbar_kws={'label': 'Accumulated Cost'},\n",
    "                         xticklabels=sp1_stages, yticklabels=sp2_stages)\n",
    "        plt.xlabel(f\"{sp1} Stages (ordered)\")\n",
    "        plt.ylabel(f\"{sp2} Stages (ordered)\")\n",
    "        plt.title(f\"DTW Alignment (stage) ({sp1} vs {sp2}): Accumulated Cost Matrix\")\n",
    "        for k in range(len(path_x) - 1):\n",
    "            x0, y0 = path_x[k], path_y[k]\n",
    "            dx = path_x[k+1] - x0\n",
    "            dy = path_y[k+1] - y0\n",
    "            plt.arrow(x0 + 0.5, y0 + 0.5, dx, dy, color='black', \n",
    "                      head_width=0.3, head_length=0.3, length_includes_head=True)\n",
    "        plt.tight_layout()\n",
    "        out_fig_stage = f\"DTW_alignment_heatmap_{sp1}_vs_{sp2}_stages.png\"\n",
    "        plt.savefig(out_fig_stage, dpi=300)\n",
    "        plt.show()\n",
    "        print(f\"保存 DTW 对齐热图 (stage) 到 {out_fig_stage}\", flush=True)\n",
    "        \n",
    "        # 额外绘制曼哈顿距离热图 (stage) -> \"Manhattan Distance\"\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        ax = sns.heatmap(stage_distance_mat, cmap='plasma',\n",
    "                         cbar_kws={'label': 'Manhattan Distance'},\n",
    "                         xticklabels=sp2_stages, yticklabels=sp1_stages)\n",
    "        plt.xlabel(f\"{sp2} Stages\")\n",
    "        plt.ylabel(f\"{sp1} Stages\")\n",
    "        plt.title(f\"Manhattan Distance Heatmap (stage): {sp1} vs {sp2}\")\n",
    "        plt.tight_layout()\n",
    "        out_fig_stage_manhattan = f\"Manhattan_distance_heatmap_{sp1}_vs_{sp2}_stages.png\"\n",
    "        plt.savefig(out_fig_stage_manhattan, dpi=300)\n",
    "        plt.show()\n",
    "        print(f\"保存曼哈顿距离热图 (stage) 到 {out_fig_stage_manhattan}\", flush=True)\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "# 5. 单独重新加载 R-AG 和 S-AG 数据，并对其进行 sample 和 stage 分组分析（AG部分）\n",
    "##############################################################################\n",
    "path_r_ag = \"../../R-AG/1.subset/R-AG_cleaned.h5ad\"\n",
    "path_s_ag = \"../../S-AG/1.subset/S-AG_cleaned.h5ad\"\n",
    "\n",
    "adata_rag = sc.read_h5ad(path_r_ag)\n",
    "adata_sag = sc.read_h5ad(path_s_ag)\n",
    "\n",
    "# 添加物种标签\n",
    "adata_rag.obs['species'] = \"R-AG\"\n",
    "adata_sag.obs['species'] = \"S-AG\"\n",
    "\n",
    "adata_dict_ag = {\"R-AG\": adata_rag, \"S-AG\": adata_sag}\n",
    "\n",
    "for grouping in [\"sample\", \"stage\"]:\n",
    "    if grouping == \"sample\" and not do_AG_sample:\n",
    "        continue\n",
    "    if grouping == \"stage\" and not do_AG_stage:\n",
    "        continue\n",
    "\n",
    "    print(f\"正在处理 R-AG vs S-AG, 分组依据: {grouping}\", flush=True)\n",
    "    \n",
    "    hvgs_r = adata_rag.var_names[adata_rag.var['highly_variable']]\n",
    "    hvgs_s = adata_sag.var_names[adata_sag.var['highly_variable']]\n",
    "    common_hvgs = list(set(hvgs_r).intersection(set(hvgs_s)))\n",
    "    print(f\"R-AG vs S-AG 共有 HVGs 数量: {len(common_hvgs)}\", flush=True)\n",
    "    \n",
    "    a_r = adata_rag[:, common_hvgs].copy()\n",
    "    a_s = adata_sag[:, common_hvgs].copy()\n",
    "    \n",
    "    adata_pair_ag = a_r.concatenate(\n",
    "         a_s,\n",
    "         batch_key=\"species_batch\",\n",
    "         batch_categories=[\"R-AG\", \"S-AG\"],\n",
    "         join='inner'\n",
    "    )\n",
    "    \n",
    "    if grouping == \"sample\":\n",
    "        #groups_r = sorted(adata_pair_ag.obs.loc[adata_pair_ag.obs['species_batch'] == \"R-AG\", 'sample'].unique().tolist())\n",
    "        groups_r = ['R-AG-E26', 'R-AG-P1''R-AG-8WK-1','R-AG-10WK-1','R-AG-25WK-1', 'R-AG-25WK-2','R-AG-GES12','R-AG-GES17','R-AG-GES23','R-AG-LA','R-AG-LA-2'\n",
    "]\n",
    "        #groups_s = sorted(adata_pair_ag.obs.loc[adata_pair_ag.obs['species_batch'] == \"S-AG\", 'sample'].unique().tolist())\n",
    "        groups_s=['S-AG-P20','S-AG-3MTH-1','S-AG-3MTH-2','S-AG-8M-2','S-AG-8M-3','S-AG-GES14','S-AG-LA-1','S-AG-LA-2']\n",
    "        group_label = \"Samples\"\n",
    "    else:\n",
    "        groups_r = sorted(adata_pair_ag.obs.loc[adata_pair_ag.obs['species_batch'] == \"R-AG\", 'stage'].unique().tolist())\n",
    "        groups_s = sorted(adata_pair_ag.obs.loc[adata_pair_ag.obs['species_batch'] == \"S-AG\", 'stage'].unique().tolist())\n",
    "        group_label = \"Stages\"\n",
    "    \n",
    "    print(f\"使用的 R-AG {group_label} 顺序: {groups_r}\", flush=True)\n",
    "    print(f\"使用的 S-AG {group_label} 顺序: {groups_s}\", flush=True)\n",
    "    \n",
    "    group_means_r = {}\n",
    "    for grp in groups_r:\n",
    "        mask = (adata_pair_ag.obs['species_batch'] == \"R-AG\") & (adata_pair_ag.obs[grouping] == grp)\n",
    "        group_means_r[grp] = np.asarray(adata_pair_ag[mask].X.mean(axis=0)).flatten()\n",
    "    \n",
    "    group_means_s = {}\n",
    "    for grp in groups_s:\n",
    "        mask = (adata_pair_ag.obs['species_batch'] == \"S-AG\") & (adata_pair_ag.obs[grouping] == grp)\n",
    "        group_means_s[grp] = np.asarray(adata_pair_ag[mask].X.mean(axis=0)).flatten()\n",
    "    \n",
    "    distance_mat_ag = pd.DataFrame(index=groups_r, columns=groups_s, dtype=float)\n",
    "    for grp_r in groups_r:\n",
    "        for grp_s in groups_s:\n",
    "            vec_r = group_means_r[grp_r]\n",
    "            vec_s = group_means_s[grp_s]\n",
    "            manhattan_dist = np.sum(np.abs(vec_r - vec_s))\n",
    "            distance_mat_ag.loc[grp_r, grp_s] = manhattan_dist\n",
    "    \n",
    "    out_csv_ag = f\"Manhattan_distance_R-AG_vs_S-AG_{grouping}.csv\"\n",
    "    distance_mat_ag.to_csv(out_csv_ag)\n",
    "    print(f\"保存曼哈顿距离矩阵 (R-AG vs S-AG, {grouping}) 到 {out_csv_ag}\", flush=True)\n",
    "    \n",
    "    # DTW 分析\n",
    "    seq_x = np.arange(len(groups_r))\n",
    "    seq_y = np.arange(len(groups_s))\n",
    "    \n",
    "    def cost_func_ag(x, y):\n",
    "        i = int(x)\n",
    "        j = int(y)\n",
    "        return distance_mat_ag.iloc[i, j]\n",
    "    \n",
    "    dtw_distance_ag, dtw_cost_mat_ag, dtw_acc_cost_mat_ag, dtw_path_ag = dtw(seq_x, seq_y, dist=cost_func_ag)\n",
    "    path_x_ag, path_y_ag = dtw_path_ag\n",
    "    print(f\"DTW 距离 (R-AG vs S-AG, {grouping}):\", dtw_distance_ag)\n",
    "    \n",
    "    # 绘制 Accumulated Cost 矩阵 (AG)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    ax = sns.heatmap(dtw_acc_cost_mat_ag.T, cmap='viridis',\n",
    "                     cbar_kws={'label': 'Accumulated Cost'},\n",
    "                     xticklabels=groups_r, yticklabels=groups_s)\n",
    "    plt.xlabel(f\"R-AG {group_label} (ordered)\")\n",
    "    plt.ylabel(f\"S-AG {group_label} (ordered)\")\n",
    "    plt.title(f\"DTW Alignment (R-AG vs S-AG, {grouping}): Accumulated Cost Matrix\")\n",
    "    for k in range(len(path_x_ag) - 1):\n",
    "        x0, y0 = path_x_ag[k], path_y_ag[k]\n",
    "        dx = path_x_ag[k+1] - x0\n",
    "        dy = path_y_ag[k+1] - y0\n",
    "        plt.arrow(x0+0.5, y0+0.5, dx, dy, color='black', head_width=0.3, head_length=0.3, length_includes_head=True)\n",
    "    plt.tight_layout()\n",
    "    out_fig_ag = f\"DTW_alignment_heatmap_R-AG_vs_S-AG_{grouping}.png\"\n",
    "    plt.savefig(out_fig_ag, dpi=300)\n",
    "    plt.show()\n",
    "    print(f\"保存 DTW 对齐热图 (R-AG vs S-AG, {grouping}) 到 {out_fig_ag}\", flush=True)\n",
    "    \n",
    "    # 额外绘制Manhattan Distance热图 (AG)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    ax = sns.heatmap(distance_mat_ag, cmap='plasma',\n",
    "                     cbar_kws={'label': 'Manhattan Distance'},\n",
    "                     xticklabels=groups_s, yticklabels=groups_r)\n",
    "    plt.xlabel(f\"S-AG {group_label}\")\n",
    "    plt.ylabel(f\"R-AG {group_label}\")\n",
    "    plt.title(f\"Manhattan Distance Heatmap (R-AG vs S-AG, {grouping})\")\n",
    "    plt.tight_layout()\n",
    "    out_fig_ag_manhattan = f\"Manhattan_distance_heatmap_R-AG_vs_S-AG_{grouping}.png\"\n",
    "    plt.savefig(out_fig_ag_manhattan, dpi=300)\n",
    "    plt.show()\n",
    "    print(f\"保存曼哈顿距离热图 (R-AG vs S-AG, {grouping}) 到 {out_fig_ag_manhattan}\", flush=True)\n",
    "\n",
    "print(\"所有计算完成！\", flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98512d5a-6392-4722-ad11-7938c50954b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1230aa89-11b9-4698-8012-00da7609b52c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 Scanpy",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
